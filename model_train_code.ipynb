{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.443794Z",
     "iopub.status.busy": "2025-04-14T17:46:54.443538Z",
     "iopub.status.idle": "2025-04-14T17:46:54.449188Z",
     "shell.execute_reply": "2025-04-14T17:46:54.448144Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.443776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Check CUDA availability and compatibility\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.450741Z",
     "iopub.status.busy": "2025-04-14T17:46:54.450537Z",
     "iopub.status.idle": "2025-04-14T17:46:54.476882Z",
     "shell.execute_reply": "2025-04-14T17:46:54.476168Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.450726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_capability())  # Should show (7,5) for T4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "73dae44e-b81a-4f64-985d-d1880334aed9",
    "_uuid": "fd5c97c7-530a-4131-976e-b0444ae2aa3c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.477894Z",
     "iopub.status.busy": "2025-04-14T17:46:54.477652Z",
     "iopub.status.idle": "2025-04-14T17:46:54.495808Z",
     "shell.execute_reply": "2025-04-14T17:46:54.49517Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.477874Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoFeatureExtractor\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==================== TEXT PROCESSING MODULE ====================\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"distilbert-base-uncased\", hidden_size=768):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model).to(device)\n",
    "\n",
    "        # â„ï¸ Freeze BERT\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.fc = nn.Linear(hidden_size, 256).to(device)\n",
    "        \n",
    "    def forward(self, texts):\n",
    "        # Tokenize texts\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        # Get text embeddings\n",
    "        outputs = self.model(**inputs)\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
    "        text_features = self.fc(text_features)\n",
    "        return text_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.49787Z",
     "iopub.status.busy": "2025-04-14T17:46:54.497284Z",
     "iopub.status.idle": "2025-04-14T17:46:54.512494Z",
     "shell.execute_reply": "2025-04-14T17:46:54.511828Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.497852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== AUDIO ENCODER USING HUBERT ====================\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import HUBERT_BASE\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=256):  # input_dim kept for compatibility\n",
    "        super(AudioEncoder, self).__init__()\n",
    "\n",
    "        # Load HuBERT base model from torchaudio\n",
    "        self.hubert_bundle = HUBERT_BASE\n",
    "        self.hubert = self.hubert_bundle.get_model().to(device)\n",
    "\n",
    "        # Freeze HuBERT parameters (can be unfrozen later for fine-tuning)\n",
    "        for param in self.hubert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Projection layer: HuBERT output (768-dim) -> hidden_dim\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(self.hubert_bundle._params['encoder_embed_dim'], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waveforms: Tensor [B, T] or [B, 1, T] (mono)\n",
    "        Returns:\n",
    "            Tensor: [B, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Ensure waveforms are [B, T]\n",
    "        if waveforms.dim() == 3 and waveforms.shape[1] == 1:\n",
    "            waveforms = waveforms.squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features, _ = self.hubert(waveforms)  # [B, T', 768]\n",
    "            pooled = features.mean(dim=1)         # [B, 768]\n",
    "\n",
    "        return self.project(pooled)               # [B, hidden_dim]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_spectrogram(audio_path, target_sr=16000, fixed_len=16000):\n",
    "        \"\"\"\n",
    "        Preprocess .wav audio file into fixed-length waveform tensor.\n",
    "        Args:\n",
    "            audio_path (str): Path to a .wav file\n",
    "            target_sr (int): Target sampling rate\n",
    "            fixed_len (int): Desired number of samples (default: 16000 = 1 second)\n",
    "        Returns:\n",
    "            waveform (Tensor): [1, fixed_len], float32, 16kHz mono\n",
    "        \"\"\"\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "        # Mono conversion\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "        # Resample to 16kHz\n",
    "        if sr != target_sr:\n",
    "            resample = Resample(orig_freq=sr, new_freq=target_sr)\n",
    "            waveform = resample(waveform)\n",
    "    \n",
    "        # Pad or truncate to fixed_len\n",
    "        num_samples = waveform.shape[1]\n",
    "        if num_samples < fixed_len:\n",
    "            pad_size = fixed_len - num_samples\n",
    "            waveform = F.pad(waveform, (0, pad_size))\n",
    "        elif num_samples > fixed_len:\n",
    "            waveform = waveform[:, :fixed_len]\n",
    "    \n",
    "        return waveform.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.513466Z",
     "iopub.status.busy": "2025-04-14T17:46:54.513217Z",
     "iopub.status.idle": "2025-04-14T17:46:54.530662Z",
     "shell.execute_reply": "2025-04-14T17:46:54.529975Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.513446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should show 'Tesla T4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.53165Z",
     "iopub.status.busy": "2025-04-14T17:46:54.531423Z",
     "iopub.status.idle": "2025-04-14T17:46:54.542517Z",
     "shell.execute_reply": "2025-04-14T17:46:54.541886Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.531623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.600417Z",
     "iopub.status.busy": "2025-04-14T17:46:54.600224Z",
     "iopub.status.idle": "2025-04-14T17:46:54.613135Z",
     "shell.execute_reply": "2025-04-14T17:46:54.612287Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.600402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(VideoEncoder, self).__init__()\n",
    "        \n",
    "        # Load pretrained Xception backbone (no classifier)\n",
    "        self.backbone = timm.create_model('xception', pretrained=True, num_classes=0).to(device)\n",
    "        \n",
    "        # Freeze all backbone parameters\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Projection layer\n",
    "        self.fc = nn.Linear(2048, hidden_dim)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 5:\n",
    "            x = x.squeeze(1)  # [B, 1, 3, H, W] â†’ [B, 3, H, W]\n",
    "        features = self.backbone(x)           # [B, 2048]\n",
    "        return self.fc(features)              # [B, hidden_dim]\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_face_features(video_file, num_frames=16):\n",
    "        \"\"\"Extract facial features from video frames\"\"\"\n",
    "        # Initialize face detector\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        \n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        \n",
    "        # Get video properties\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Calculate frame indices to extract\n",
    "        indices = np.linspace(0, frame_count-1, num_frames, dtype=int)\n",
    "        \n",
    "        # Initialize tensor to store face frames\n",
    "        face_frames = torch.zeros((num_frames, 3, 224, 224), device=device)\n",
    "\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            # Set frame position\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            \n",
    "            # Read frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            \n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "            \n",
    "            if len(faces) > 0:\n",
    "                # Extract the largest face\n",
    "                x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])\n",
    "                \n",
    "                # Expand bounding box slightly\n",
    "                x = max(0, x - int(0.1 * w))\n",
    "                y = max(0, y - int(0.1 * h))\n",
    "                w = min(frame.shape[1] - x, int(1.2 * w))\n",
    "                h = min(frame.shape[0] - y, int(1.2 * h))\n",
    "                \n",
    "                # Extract face\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                \n",
    "                # Resize to 224x224\n",
    "                face = cv2.resize(face, (224, 224))\n",
    "                \n",
    "                # Convert to RGB\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                face_tensor = torchvision.transforms.ToTensor()(face)\n",
    "                \n",
    "                # Normalize\n",
    "                face_tensor = torchvision.transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]\n",
    "                )(face_tensor).to(device)\n",
    "                \n",
    "                # Store tensor\n",
    "                face_frames[i] = face_tensor\n",
    "        \n",
    "        # Release video capture\n",
    "        cap.release()\n",
    "        \n",
    "        # Return the mean face features across frames with correct shape\n",
    "        # This will ensure shape is [1, 3, 224, 224]\n",
    "        return face_frames.mean(dim=0).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.614586Z",
     "iopub.status.busy": "2025-04-14T17:46:54.614405Z",
     "iopub.status.idle": "2025-04-14T17:46:54.633411Z",
     "shell.execute_reply": "2025-04-14T17:46:54.632824Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.614572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==================== ATTENTION FUSION MODULE ====================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=256, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        # For PyTorch's MultiheadAttention, inputs should be: [seq_len, batch_size, embed_dim]\n",
    "        # Make sure all inputs are correctly shaped\n",
    "        if query.dim() == 2:\n",
    "            query = query.unsqueeze(0)  # [1, batch_size, embed_dim]\n",
    "        else:\n",
    "            query = query.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n",
    "            \n",
    "        if key.dim() == 2:\n",
    "            key = key.unsqueeze(0)  # [1, batch_size, embed_dim]\n",
    "        elif key.dim() == 3:\n",
    "            key = key.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        if value.dim() == 2:\n",
    "            value = value.unsqueeze(0)  # [1, batch_size, embed_dim]\n",
    "        elif value.dim() == 3:\n",
    "            value = value.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n",
    "\n",
    "\n",
    "        query = query.to(self.device)\n",
    "        key = key.to(self.device)\n",
    "        value = value.to(self.device)\n",
    "        \n",
    "        # Apply multihead attention\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        \n",
    "        # Return to original shape: [batch_size, embed_dim]\n",
    "        return attn_output.transpose(0, 1).squeeze(0)\n",
    "\n",
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.mha = MultiHeadAttention(d_model=hidden_dim).to(self.device)\n",
    "        \n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "        # Create sequence for key and value (3 Ãƒâ€” [batch_size, feature_dim])\n",
    "        # -> [3, batch_size, feature_dim]\n",
    "        features = torch.stack([text_features.to(self.device), audio_features.to(self.device), video_features.to(self.device)], dim=0)        \n",
    "        # Apply attention to each feature vector as query\n",
    "        # We need to ensure features tensor is correctly shaped for the attention mechanism\n",
    "        text_attn = self.mha(text_features, features, features)\n",
    "        audio_attn = self.mha(audio_features, features, features)\n",
    "        video_attn = self.mha(video_features, features, features)\n",
    "        \n",
    "        # Combine attended features\n",
    "        fused_features = (text_attn + audio_attn + video_attn) / 3\n",
    "        \n",
    "        return fused_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.634451Z",
     "iopub.status.busy": "2025-04-14T17:46:54.634203Z",
     "iopub.status.idle": "2025-04-14T17:46:54.654007Z",
     "shell.execute_reply": "2025-04-14T17:46:54.653137Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.634432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimplerAttentionFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(SimplerAttentionFusion, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_dim, 3)\n",
    "        \n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "        # Stack features along a new dimension\n",
    "        features = torch.stack([text_features, audio_features, video_features], dim=1)  # [batch_size, 3, hidden_dim]\n",
    "        \n",
    "        # Calculate attention weights (simplified attention)\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        # Use the mean of all features as a query\n",
    "        query = features.mean(dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention_scores = self.attention_weights(query)  # [batch_size, 3]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, 3, 1]\n",
    "        \n",
    "        # Apply attention weights\n",
    "        weighted_features = features * attention_weights  # [batch_size, 3, hidden_dim]\n",
    "        \n",
    "        # Sum over the modalities\n",
    "        fused_features = weighted_features.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        return fused_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.655826Z",
     "iopub.status.busy": "2025-04-14T17:46:54.655618Z",
     "iopub.status.idle": "2025-04-14T17:46:54.667694Z",
     "shell.execute_reply": "2025-04-14T17:46:54.66713Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.655806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_layers=2, num_heads=4):\n",
    "        super(TransformerFusion, self).__init__()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_dim*2, \n",
    "            dropout=0.1, \n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.modality_embeddings = nn.Parameter(torch.randn(3, hidden_dim))  # For modality type info\n",
    "\n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "        batch_size = text_features.size(0)\n",
    "        \n",
    "        # Stack features with modality tokens\n",
    "        features = torch.stack([text_features, audio_features, video_features], dim=1)  # [B, 3, D]\n",
    "        \n",
    "        # Add modality embeddings\n",
    "        features = features + self.modality_embeddings.unsqueeze(0)  # [B, 3, D]\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        fused = self.transformer_encoder(features)  # [B, 3, D]\n",
    "        \n",
    "        # Pooling: mean over modalities\n",
    "        fused_output = fused.mean(dim=1)  # [B, D]\n",
    "        \n",
    "        return fused_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.668544Z",
     "iopub.status.busy": "2025-04-14T17:46:54.668321Z",
     "iopub.status.idle": "2025-04-14T17:46:54.686548Z",
     "shell.execute_reply": "2025-04-14T17:46:54.685875Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.668525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== FULL MODEL ====================\n",
    "class MultimodalEmotionRecognition(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(MultimodalEmotionRecognition, self).__init__()\n",
    "        \n",
    "        # Encoders for each modality\n",
    "        self.text_encoder = TextEncoder(hidden_size=768, pretrained_model=\"distilbert-base-uncased\")\n",
    "        self.audio_encoder = AudioEncoder(hidden_dim=hidden_dim)\n",
    "        self.video_encoder = VideoEncoder(hidden_dim=hidden_dim)\n",
    "\n",
    "         # Use the simpler attention fusion module\n",
    "        #self.fusion = SimplerAttentionFusion(hidden_dim=hidden_dim)\n",
    "        self.fusion = TransformerFusion(hidden_dim=hidden_dim)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # Binary classification for surprise\n",
    "        \n",
    "    def forward(self, texts, audio_specs, video_frames):\n",
    "        # Fix video input dimensions if needed\n",
    "        if video_frames.dim() == 5:  # [batch_size, 1, 3, height, width]\n",
    "            video_frames = video_frames.squeeze(1)\n",
    "            \n",
    "        # Encode each modality\n",
    "        text_features = self.text_encoder(texts)\n",
    "        audio_features = self.audio_encoder(audio_specs)\n",
    "        video_features = self.video_encoder(video_frames)\n",
    "        \n",
    "        # Fuse features using attention\n",
    "        fused_features = self.fusion(text_features, audio_features, video_features)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.fc(fused_features)\n",
    "        output = torch.sigmoid(output)  # Probability of surprise emotion\n",
    "        \n",
    "        # Ensure output maintains proper dimensions for batch size 1\n",
    "        batch_size = text_features.size(0)\n",
    "        output = output.view(batch_size)  # Reshape to [batch_size]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.687497Z",
     "iopub.status.busy": "2025-04-14T17:46:54.687295Z",
     "iopub.status.idle": "2025-04-14T17:46:54.702653Z",
     "shell.execute_reply": "2025-04-14T17:46:54.701951Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.687474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== DATASET CLASS ====================\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "        \n",
    "    def _load_samples(self):\n",
    "        \"\"\"Load sample paths and labels from the data directory\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        # Assuming directory structure:\n",
    "        # data_dir/\n",
    "        #   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ sample1/\n",
    "        #   Ã¢â€â€š   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ text.txt\n",
    "        #   Ã¢â€â€š   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ audio.wav\n",
    "        #   Ã¢â€â€š   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ video.mp4\n",
    "        #   Ã¢â€â€š   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬ label.txt\n",
    "        #   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬ sample2/\n",
    "        #       Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ ...\n",
    "        \n",
    "        for sample_dir in os.listdir(self.data_dir):\n",
    "            sample_path = os.path.join(self.data_dir, sample_dir)\n",
    "            if os.path.isdir(sample_path):\n",
    "                text_path = os.path.join(sample_path, \"text.txt\")\n",
    "                audio_path = os.path.join(sample_path, \"audio.wav\")\n",
    "                video_path = os.path.join(sample_path, \"video.mp4\")\n",
    "                label_path = os.path.join(sample_path, \"label.txt\")\n",
    "                \n",
    "                if os.path.exists(text_path) and os.path.exists(audio_path) and \\\n",
    "                   os.path.exists(video_path) and os.path.exists(label_path):\n",
    "                    with open(text_path, 'r') as f:\n",
    "                        text = f.read().strip()\n",
    "                    \n",
    "                    with open(label_path, 'r') as f:\n",
    "                        # Assuming the label file contains 1 for surprise, 0 for no surprise\n",
    "                        label = int(f.read().strip())\n",
    "                    \n",
    "                    samples.append({\n",
    "                        \"text\": text,\n",
    "                        \"audio_path\": audio_path,\n",
    "                        \"video_path\": video_path,\n",
    "                        \"label\": label\n",
    "                    })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Get text\n",
    "        text = sample[\"text\"]\n",
    "        \n",
    "        # Extract audio features\n",
    "        audio_spec = AudioEncoder.extract_spectrogram(sample[\"audio_path\"])\n",
    "        \n",
    "        # Extract video features\n",
    "        video_features = VideoEncoder.extract_face_features(sample[\"video_path\"])\n",
    "\n",
    "        # Ensure video features have correct shape [1, 3, 224, 224]\n",
    "        if video_features.dim() > 4:\n",
    "            video_features = video_features.squeeze(1)\n",
    "        \n",
    "        # Get label\n",
    "        label = torch.tensor(sample[\"label\"], dtype=torch.float32)\n",
    "        \n",
    "        return text, audio_spec, video_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.703949Z",
     "iopub.status.busy": "2025-04-14T17:46:54.703407Z",
     "iopub.status.idle": "2025-04-14T17:46:54.716198Z",
     "shell.execute_reply": "2025-04-14T17:46:54.715607Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.703913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.718078Z",
     "iopub.status.busy": "2025-04-14T17:46:54.717866Z",
     "iopub.status.idle": "2025-04-14T17:46:54.737112Z",
     "shell.execute_reply": "2025-04-14T17:46:54.736431Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.718063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time  # Added import at the top\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    # Timing setup\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)  # Move model to GPU\n",
    "\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()  # Start epoch timer\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for texts, audio_specs, video_frames, labels in train_loader:\n",
    "            # Move data to device\n",
    "            # texts = texts.to(device)\n",
    "            audio_specs = audio_specs.to(device)\n",
    "            video_frames = video_frames.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts, audio_specs, video_frames)\n",
    "            \n",
    "            # Ensure dimensions match\n",
    "            if outputs.dim() == 0:\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "            if outputs.dim() > 1:\n",
    "                outputs = outputs.view(labels.size())\n",
    "                \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, audio_specs, video_frames, labels in val_loader:\n",
    "                # texts = texts.to(device)\n",
    "                audio_specs = audio_specs.to(device)\n",
    "                video_frames = video_frames.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(texts, audio_specs, video_frames)\n",
    "                \n",
    "                if outputs.dim() == 0:\n",
    "                    outputs = outputs.unsqueeze(0)\n",
    "                if outputs.dim() > 1:\n",
    "                    outputs = outputs.view(labels.size())\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        # Print epoch statistics with timing\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} completed in {time.strftime(\"%H:%M:%S\", time.gmtime(epoch_time))}\\n'\n",
    "              f'  Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Accuracy: {val_accuracy:.4f}\\n')\n",
    "    \n",
    "    # Final timing and output\n",
    "    total_time = time.time() - total_start\n",
    "    print(f'\\nðŸ”¥ Total training completed in {time.strftime(\"%H:%M:%S\", time.gmtime(total_time))}')\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('training_curve.png')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.737947Z",
     "iopub.status.busy": "2025-04-14T17:46:54.737716Z",
     "iopub.status.idle": "2025-04-14T17:46:54.752951Z",
     "shell.execute_reply": "2025-04-14T17:46:54.752304Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.737914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.754175Z",
     "iopub.status.busy": "2025-04-14T17:46:54.753911Z",
     "iopub.status.idle": "2025-04-14T17:46:54.764121Z",
     "shell.execute_reply": "2025-04-14T17:46:54.763522Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.754154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_emotion(model, text, audio_path, video_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)  # Move model to GPU before prediction\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Preprocess inputs\n",
    "            audio_spec = AudioEncoder.extract_spectrogram(audio_path).to(device)\n",
    "            video_features = VideoEncoder.extract_face_features(video_path).to(device)\n",
    "            \n",
    "            # Ensure proper batch dimension\n",
    "            if audio_spec.dim() == 3:  # [channel, height, width]\n",
    "                audio_spec = audio_spec.unsqueeze(0)  # Add batch dimension -> [batch_size, channel, height, width]\n",
    "                \n",
    "            if video_features.dim() == 3:  # [channel, height, width]\n",
    "                video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model([text], audio_spec, video_features)\n",
    "            \n",
    "            # Get prediction\n",
    "            if output.dim() == 0:  # If scalar\n",
    "                probability = output.item()\n",
    "            else:\n",
    "                probability = output.squeeze().item()\n",
    "                \n",
    "            prediction = \"Surprise\" if probability > 0.5 else \"Not Surprise\"\n",
    "            \n",
    "            return prediction, probability\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            print(f\"Audio spec shape: {audio_spec.shape}\")\n",
    "            print(f\"Video features shape: {video_features.shape}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.764873Z",
     "iopub.status.busy": "2025-04-14T17:46:54.764725Z",
     "iopub.status.idle": "2025-04-14T17:46:54.779583Z",
     "shell.execute_reply": "2025-04-14T17:46:54.778977Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.764861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")  # Should return True[4][5]\n",
    "print(f\"Detected GPU: {torch.cuda.get_device_name(0)}\")  # Should show \"Tesla T4\"[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:46:54.781547Z",
     "iopub.status.busy": "2025-04-14T17:46:54.78106Z",
     "iopub.status.idle": "2025-04-14T18:24:17.7367Z",
     "shell.execute_reply": "2025-04-14T18:24:17.735898Z",
     "shell.execute_reply.started": "2025-04-14T17:46:54.781531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== MAIN CODE ====================\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create model\n",
    "model = MultimodalEmotionRecognition()\n",
    "model = model.to(device)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "print(f\"Model on GPU: {next(model.parameters()).is_cuda}\")\n",
    "    \n",
    "    \n",
    "    # After model creation\n",
    "print(\"Model device check:\")\n",
    "print(f\"Text encoder: {next(model.text_encoder.parameters()).device}\")\n",
    "print(f\"Audio encoder: {next(model.audio_encoder.parameters()).device}\")\n",
    "print(f\"Video encoder: {next(model.video_encoder.parameters()).device}\")\n",
    "\n",
    "# 1. Training\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MultimodalEmotionDataset('/dataset/final_train')\n",
    "val_dataset = MultimodalEmotionDataset('/dataset/final_val')\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "model = train_model(model, train_loader, val_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T18:24:17.737929Z",
     "iopub.status.busy": "2025-04-14T18:24:17.737651Z",
     "iopub.status.idle": "2025-04-14T18:24:17.79664Z",
     "shell.execute_reply": "2025-04-14T18:24:17.79521Z",
     "shell.execute_reply.started": "2025-04-14T18:24:17.737896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-14T18:24:17.797295Z",
     "iopub.status.idle": "2025-04-14T18:24:17.797564Z",
     "shell.execute_reply": "2025-04-14T18:24:17.797464Z",
     "shell.execute_reply.started": "2025-04-14T18:24:17.797452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7105665,
     "sourceId": 11354577,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7106436,
     "sourceId": 11355543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7111431,
     "sourceId": 11362096,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7111456,
     "sourceId": 11362124,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7111513,
     "sourceId": 11362214,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7111524,
     "sourceId": 11362226,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7111751,
     "sourceId": 11362554,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7111943,
     "sourceId": 11362823,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7111951,
     "sourceId": 11362833,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7133510,
     "sourceId": 11390867,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7141398,
     "sourceId": 11401824,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7141407,
     "sourceId": 11401835,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
