{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11354577,"sourceType":"datasetVersion","datasetId":7105665},{"sourceId":11355543,"sourceType":"datasetVersion","datasetId":7106436},{"sourceId":11362096,"sourceType":"datasetVersion","datasetId":7111431},{"sourceId":11362124,"sourceType":"datasetVersion","datasetId":7111456},{"sourceId":11362214,"sourceType":"datasetVersion","datasetId":7111513},{"sourceId":11362226,"sourceType":"datasetVersion","datasetId":7111524},{"sourceId":11362554,"sourceType":"datasetVersion","datasetId":7111751},{"sourceId":11362823,"sourceType":"datasetVersion","datasetId":7111943},{"sourceId":11362833,"sourceType":"datasetVersion","datasetId":7111951},{"sourceId":11390867,"sourceType":"datasetVersion","datasetId":7133510},{"sourceId":11401824,"sourceType":"datasetVersion","datasetId":7141398},{"sourceId":11401835,"sourceType":"datasetVersion","datasetId":7141407}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n# Check CUDA availability and compatibility\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.443538Z","iopub.execute_input":"2025-04-14T17:46:54.443794Z","iopub.status.idle":"2025-04-14T17:46:54.449188Z","shell.execute_reply.started":"2025-04-14T17:46:54.443776Z","shell.execute_reply":"2025-04-14T17:46:54.448144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.get_device_capability())  # Should show (7,5) for T4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.450537Z","iopub.execute_input":"2025-04-14T17:46:54.450741Z","iopub.status.idle":"2025-04-14T17:46:54.476882Z","shell.execute_reply.started":"2025-04-14T17:46:54.450726Z","shell.execute_reply":"2025-04-14T17:46:54.476168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer, AutoFeatureExtractor\nimport torchaudio\nimport torchvision\nimport numpy as np\nimport os\nimport cv2\nfrom PIL import Image\nimport librosa\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Check for CUDA availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ==================== TEXT PROCESSING MODULE ====================\nclass TextEncoder(nn.Module):\n    def __init__(self, pretrained_model=\"distilbert-base-uncased\", hidden_size=768):\n        super(TextEncoder, self).__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n        self.model = AutoModel.from_pretrained(pretrained_model).to(device)\n\n        # ❄️ Freeze BERT\n        for param in self.model.parameters():\n            param.requires_grad = False\n            \n        self.fc = nn.Linear(hidden_size, 256).to(device)\n        \n    def forward(self, texts):\n        # Tokenize texts\n        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n        inputs = {key: val.to(device) for key, val in inputs.items()}\n        \n        # Get text embeddings\n        outputs = self.model(**inputs)\n        text_features = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n        text_features = self.fc(text_features)\n        return text_features\n\n","metadata":{"_uuid":"fd5c97c7-530a-4131-976e-b0444ae2aa3c","_cell_guid":"73dae44e-b81a-4f64-985d-d1880334aed9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-14T17:46:54.477652Z","iopub.execute_input":"2025-04-14T17:46:54.477894Z","iopub.status.idle":"2025-04-14T17:46:54.495808Z","shell.execute_reply.started":"2025-04-14T17:46:54.477874Z","shell.execute_reply":"2025-04-14T17:46:54.49517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== AUDIO ENCODER USING HUBERT ====================\nimport torchaudio\nfrom torchaudio.pipelines import HUBERT_BASE\nfrom torchaudio.transforms import Resample\n\nclass AudioEncoder(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256):  # input_dim kept for compatibility\n        super(AudioEncoder, self).__init__()\n\n        # Load HuBERT base model from torchaudio\n        self.hubert_bundle = HUBERT_BASE\n        self.hubert = self.hubert_bundle.get_model().to(device)\n\n        # Freeze HuBERT parameters (can be unfrozen later for fine-tuning)\n        for param in self.hubert.parameters():\n            param.requires_grad = False\n\n        # Projection layer: HuBERT output (768-dim) -> hidden_dim\n        self.project = nn.Sequential(\n            nn.Linear(self.hubert_bundle._params['encoder_embed_dim'], 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, hidden_dim)\n        )\n\n        self.to(device)\n\n    def forward(self, waveforms):\n        \"\"\"\n        Args:\n            waveforms: Tensor [B, T] or [B, 1, T] (mono)\n        Returns:\n            Tensor: [B, hidden_dim]\n        \"\"\"\n        # Ensure waveforms are [B, T]\n        if waveforms.dim() == 3 and waveforms.shape[1] == 1:\n            waveforms = waveforms.squeeze(1)\n\n        with torch.no_grad():\n            features, _ = self.hubert(waveforms)  # [B, T', 768]\n            pooled = features.mean(dim=1)         # [B, 768]\n\n        return self.project(pooled)               # [B, hidden_dim]\n\n    @staticmethod\n    def extract_spectrogram(audio_path, target_sr=16000, fixed_len=16000):\n        \"\"\"\n        Preprocess .wav audio file into fixed-length waveform tensor.\n        Args:\n            audio_path (str): Path to a .wav file\n            target_sr (int): Target sampling rate\n            fixed_len (int): Desired number of samples (default: 16000 = 1 second)\n        Returns:\n            waveform (Tensor): [1, fixed_len], float32, 16kHz mono\n        \"\"\"\n        waveform, sr = torchaudio.load(audio_path)\n    \n        # Mono conversion\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n    \n        # Resample to 16kHz\n        if sr != target_sr:\n            resample = Resample(orig_freq=sr, new_freq=target_sr)\n            waveform = resample(waveform)\n    \n        # Pad or truncate to fixed_len\n        num_samples = waveform.shape[1]\n        if num_samples < fixed_len:\n            pad_size = fixed_len - num_samples\n            waveform = F.pad(waveform, (0, pad_size))\n        elif num_samples > fixed_len:\n            waveform = waveform[:, :fixed_len]\n    \n        return waveform.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.497284Z","iopub.execute_input":"2025-04-14T17:46:54.49787Z","iopub.status.idle":"2025-04-14T17:46:54.512494Z","shell.execute_reply.started":"2025-04-14T17:46:54.497852Z","shell.execute_reply":"2025-04-14T17:46:54.511828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.is_available())  # Should return True\nprint(torch.cuda.get_device_name(0))  # Should show 'Tesla T4'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.513217Z","iopub.execute_input":"2025-04-14T17:46:54.513466Z","iopub.status.idle":"2025-04-14T17:46:54.530662Z","shell.execute_reply.started":"2025-04-14T17:46:54.513446Z","shell.execute_reply":"2025-04-14T17:46:54.529975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.531423Z","iopub.execute_input":"2025-04-14T17:46:54.53165Z","iopub.status.idle":"2025-04-14T17:46:54.542517Z","shell.execute_reply.started":"2025-04-14T17:46:54.531623Z","shell.execute_reply":"2025-04-14T17:46:54.541886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import timm\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport timm\nimport torch.nn as nn\n\nclass VideoEncoder(nn.Module):\n    def __init__(self, hidden_dim=256):\n        super(VideoEncoder, self).__init__()\n        \n        # Load pretrained Xception backbone (no classifier)\n        self.backbone = timm.create_model('xception', pretrained=True, num_classes=0).to(device)\n        \n        # Freeze all backbone parameters\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n\n        # Projection layer\n        self.fc = nn.Linear(2048, hidden_dim)\n        self.to(device)\n\n    def forward(self, x):\n        if x.dim() == 5:\n            x = x.squeeze(1)  # [B, 1, 3, H, W] → [B, 3, H, W]\n        features = self.backbone(x)           # [B, 2048]\n        return self.fc(features)              # [B, hidden_dim]\n    \n    @staticmethod\n    def extract_face_features(video_file, num_frames=16):\n        \"\"\"Extract facial features from video frames\"\"\"\n        # Initialize face detector\n        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n        \n        # Open video file\n        cap = cv2.VideoCapture(video_file)\n        \n        # Get video properties\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        \n        # Calculate frame indices to extract\n        indices = np.linspace(0, frame_count-1, num_frames, dtype=int)\n        \n        # Initialize tensor to store face frames\n        face_frames = torch.zeros((num_frames, 3, 224, 224), device=device)\n\n        \n        for i, idx in enumerate(indices):\n            # Set frame position\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            \n            # Read frame\n            ret, frame = cap.read()\n            if not ret:\n                continue\n            \n            # Convert to grayscale for face detection\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            \n            # Detect faces\n            faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n            \n            if len(faces) > 0:\n                # Extract the largest face\n                x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])\n                \n                # Expand bounding box slightly\n                x = max(0, x - int(0.1 * w))\n                y = max(0, y - int(0.1 * h))\n                w = min(frame.shape[1] - x, int(1.2 * w))\n                h = min(frame.shape[0] - y, int(1.2 * h))\n                \n                # Extract face\n                face = frame[y:y+h, x:x+w]\n                \n                # Resize to 224x224\n                face = cv2.resize(face, (224, 224))\n                \n                # Convert to RGB\n                face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n                \n                # Convert to tensor\n                face_tensor = torchvision.transforms.ToTensor()(face)\n                \n                # Normalize\n                face_tensor = torchvision.transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                )(face_tensor).to(device)\n                \n                # Store tensor\n                face_frames[i] = face_tensor\n        \n        # Release video capture\n        cap.release()\n        \n        # Return the mean face features across frames with correct shape\n        # This will ensure shape is [1, 3, 224, 224]\n        return face_frames.mean(dim=0).unsqueeze(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.600224Z","iopub.execute_input":"2025-04-14T17:46:54.600417Z","iopub.status.idle":"2025-04-14T17:46:54.613135Z","shell.execute_reply.started":"2025-04-14T17:46:54.600402Z","shell.execute_reply":"2025-04-14T17:46:54.612287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ==================== ATTENTION FUSION MODULE ====================\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model=256, num_heads=4):\n        super(MultiHeadAttention, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n        \n    def forward(self, query, key, value):\n        # For PyTorch's MultiheadAttention, inputs should be: [seq_len, batch_size, embed_dim]\n        # Make sure all inputs are correctly shaped\n        if query.dim() == 2:\n            query = query.unsqueeze(0)  # [1, batch_size, embed_dim]\n        else:\n            query = query.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n            \n        if key.dim() == 2:\n            key = key.unsqueeze(0)  # [1, batch_size, embed_dim]\n        elif key.dim() == 3:\n            key = key.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n        \n        if value.dim() == 2:\n            value = value.unsqueeze(0)  # [1, batch_size, embed_dim]\n        elif value.dim() == 3:\n            value = value.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n\n\n        query = query.to(self.device)\n        key = key.to(self.device)\n        value = value.to(self.device)\n        \n        # Apply multihead attention\n        attn_output, _ = self.attention(query, key, value)\n        \n        # Return to original shape: [batch_size, embed_dim]\n        return attn_output.transpose(0, 1).squeeze(0)\n\nclass AttentionFusion(nn.Module):\n    def __init__(self, hidden_dim=256, device='cuda'):\n        super().__init__()\n        self.device = device\n        self.mha = MultiHeadAttention(d_model=hidden_dim).to(self.device)\n        \n    def forward(self, text_features, audio_features, video_features):\n        # Create sequence for key and value (3 Ã— [batch_size, feature_dim])\n        # -> [3, batch_size, feature_dim]\n        features = torch.stack([text_features.to(self.device), audio_features.to(self.device), video_features.to(self.device)], dim=0)        \n        # Apply attention to each feature vector as query\n        # We need to ensure features tensor is correctly shaped for the attention mechanism\n        text_attn = self.mha(text_features, features, features)\n        audio_attn = self.mha(audio_features, features, features)\n        video_attn = self.mha(video_features, features, features)\n        \n        # Combine attended features\n        fused_features = (text_attn + audio_attn + video_attn) / 3\n        \n        return fused_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.614405Z","iopub.execute_input":"2025-04-14T17:46:54.614586Z","iopub.status.idle":"2025-04-14T17:46:54.633411Z","shell.execute_reply.started":"2025-04-14T17:46:54.614572Z","shell.execute_reply":"2025-04-14T17:46:54.632824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimplerAttentionFusion(nn.Module):\n    def __init__(self, hidden_dim=256):\n        super(SimplerAttentionFusion, self).__init__()\n        self.attention_weights = nn.Linear(hidden_dim, 3)\n        \n    def forward(self, text_features, audio_features, video_features):\n        # Stack features along a new dimension\n        features = torch.stack([text_features, audio_features, video_features], dim=1)  # [batch_size, 3, hidden_dim]\n        \n        # Calculate attention weights (simplified attention)\n        batch_size = features.size(0)\n        \n        # Use the mean of all features as a query\n        query = features.mean(dim=1)  # [batch_size, hidden_dim]\n        \n        # Calculate attention scores\n        attention_scores = self.attention_weights(query)  # [batch_size, 3]\n        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, 3, 1]\n        \n        # Apply attention weights\n        weighted_features = features * attention_weights  # [batch_size, 3, hidden_dim]\n        \n        # Sum over the modalities\n        fused_features = weighted_features.sum(dim=1)  # [batch_size, hidden_dim]\n        \n        return fused_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.634203Z","iopub.execute_input":"2025-04-14T17:46:54.634451Z","iopub.status.idle":"2025-04-14T17:46:54.654007Z","shell.execute_reply.started":"2025-04-14T17:46:54.634432Z","shell.execute_reply":"2025-04-14T17:46:54.653137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerFusion(nn.Module):\n    def __init__(self, hidden_dim=256, num_layers=2, num_heads=4):\n        super(TransformerFusion, self).__init__()\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, \n            nhead=num_heads, \n            dim_feedforward=hidden_dim*2, \n            dropout=0.1, \n            activation='relu',\n            batch_first=True\n        )\n        \n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.modality_embeddings = nn.Parameter(torch.randn(3, hidden_dim))  # For modality type info\n\n    def forward(self, text_features, audio_features, video_features):\n        batch_size = text_features.size(0)\n        \n        # Stack features with modality tokens\n        features = torch.stack([text_features, audio_features, video_features], dim=1)  # [B, 3, D]\n        \n        # Add modality embeddings\n        features = features + self.modality_embeddings.unsqueeze(0)  # [B, 3, D]\n        \n        # Pass through Transformer Encoder\n        fused = self.transformer_encoder(features)  # [B, 3, D]\n        \n        # Pooling: mean over modalities\n        fused_output = fused.mean(dim=1)  # [B, D]\n        \n        return fused_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.655618Z","iopub.execute_input":"2025-04-14T17:46:54.655826Z","iopub.status.idle":"2025-04-14T17:46:54.667694Z","shell.execute_reply.started":"2025-04-14T17:46:54.655806Z","shell.execute_reply":"2025-04-14T17:46:54.66713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== FULL MODEL ====================\nclass MultimodalEmotionRecognition(nn.Module):\n    def __init__(self, hidden_dim=256):\n        super(MultimodalEmotionRecognition, self).__init__()\n        \n        # Encoders for each modality\n        self.text_encoder = TextEncoder(hidden_size=768, pretrained_model=\"distilbert-base-uncased\")\n        self.audio_encoder = AudioEncoder(hidden_dim=hidden_dim)\n        self.video_encoder = VideoEncoder(hidden_dim=hidden_dim)\n\n         # Use the simpler attention fusion module\n        #self.fusion = SimplerAttentionFusion(hidden_dim=hidden_dim)\n        self.fusion = TransformerFusion(hidden_dim=hidden_dim)\n        \n        # Final classification layer\n        self.fc = nn.Linear(hidden_dim, 1)  # Binary classification for surprise\n        \n    def forward(self, texts, audio_specs, video_frames):\n        # Fix video input dimensions if needed\n        if video_frames.dim() == 5:  # [batch_size, 1, 3, height, width]\n            video_frames = video_frames.squeeze(1)\n            \n        # Encode each modality\n        text_features = self.text_encoder(texts)\n        audio_features = self.audio_encoder(audio_specs)\n        video_features = self.video_encoder(video_frames)\n        \n        # Fuse features using attention\n        fused_features = self.fusion(text_features, audio_features, video_features)\n        \n        # Classification\n        output = self.fc(fused_features)\n        output = torch.sigmoid(output)  # Probability of surprise emotion\n        \n        # Ensure output maintains proper dimensions for batch size 1\n        batch_size = text_features.size(0)\n        output = output.view(batch_size)  # Reshape to [batch_size]\n        \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.668321Z","iopub.execute_input":"2025-04-14T17:46:54.668544Z","iopub.status.idle":"2025-04-14T17:46:54.686548Z","shell.execute_reply.started":"2025-04-14T17:46:54.668525Z","shell.execute_reply":"2025-04-14T17:46:54.685875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== DATASET CLASS ====================\nclass MultimodalEmotionDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.samples = self._load_samples()\n        \n    def _load_samples(self):\n        \"\"\"Load sample paths and labels from the data directory\"\"\"\n        samples = []\n        \n        # Assuming directory structure:\n        # data_dir/\n        #   â”œâ”€â”€ sample1/\n        #   â”‚   â”œâ”€â”€ text.txt\n        #   â”‚   â”œâ”€â”€ audio.wav\n        #   â”‚   â”œâ”€â”€ video.mp4\n        #   â”‚   â””â”€â”€ label.txt\n        #   â””â”€â”€ sample2/\n        #       â”œâ”€â”€ ...\n        \n        for sample_dir in os.listdir(self.data_dir):\n            sample_path = os.path.join(self.data_dir, sample_dir)\n            if os.path.isdir(sample_path):\n                text_path = os.path.join(sample_path, \"text.txt\")\n                audio_path = os.path.join(sample_path, \"audio.wav\")\n                video_path = os.path.join(sample_path, \"video.mp4\")\n                label_path = os.path.join(sample_path, \"label.txt\")\n                \n                if os.path.exists(text_path) and os.path.exists(audio_path) and \\\n                   os.path.exists(video_path) and os.path.exists(label_path):\n                    with open(text_path, 'r') as f:\n                        text = f.read().strip()\n                    \n                    with open(label_path, 'r') as f:\n                        # Assuming the label file contains 1 for surprise, 0 for no surprise\n                        label = int(f.read().strip())\n                    \n                    samples.append({\n                        \"text\": text,\n                        \"audio_path\": audio_path,\n                        \"video_path\": video_path,\n                        \"label\": label\n                    })\n        \n        return samples\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        \n        # Get text\n        text = sample[\"text\"]\n        \n        # Extract audio features\n        audio_spec = AudioEncoder.extract_spectrogram(sample[\"audio_path\"])\n        \n        # Extract video features\n        video_features = VideoEncoder.extract_face_features(sample[\"video_path\"])\n\n        # Ensure video features have correct shape [1, 3, 224, 224]\n        if video_features.dim() > 4:\n            video_features = video_features.squeeze(1)\n        \n        # Get label\n        label = torch.tensor(sample[\"label\"], dtype=torch.float32)\n        \n        return text, audio_spec, video_features, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.687295Z","iopub.execute_input":"2025-04-14T17:46:54.687497Z","iopub.status.idle":"2025-04-14T17:46:54.702653Z","shell.execute_reply.started":"2025-04-14T17:46:54.687474Z","shell.execute_reply":"2025-04-14T17:46:54.701951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.703407Z","iopub.execute_input":"2025-04-14T17:46:54.703949Z","iopub.status.idle":"2025-04-14T17:46:54.716198Z","shell.execute_reply.started":"2025-04-14T17:46:54.703913Z","shell.execute_reply":"2025-04-14T17:46:54.715607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time  # Added import at the top\n\ndef train_model(model, train_loader, val_loader, num_epochs=10):\n    # Timing setup\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)  # Move model to GPU\n\n    total_start = time.time()\n    \n    # Loss function and optimizer\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n    \n    # Training loop\n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(num_epochs):\n        epoch_start = time.time()  # Start epoch timer\n        \n        # Training phase\n        model.train()\n        train_loss = 0.0\n        \n        for texts, audio_specs, video_frames, labels in train_loader:\n            # Move data to device\n            # texts = texts.to(device)\n            audio_specs = audio_specs.to(device)\n            video_frames = video_frames.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            outputs = model(texts, audio_specs, video_frames)\n            \n            # Ensure dimensions match\n            if outputs.dim() == 0:\n                outputs = outputs.unsqueeze(0)\n            if outputs.dim() > 1:\n                outputs = outputs.view(labels.size())\n                \n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for texts, audio_specs, video_frames, labels in val_loader:\n                # texts = texts.to(device)\n                audio_specs = audio_specs.to(device)\n                video_frames = video_frames.to(device)\n                labels = labels.to(device)\n                \n                outputs = model(texts, audio_specs, video_frames)\n                \n                if outputs.dim() == 0:\n                    outputs = outputs.unsqueeze(0)\n                if outputs.dim() > 1:\n                    outputs = outputs.view(labels.size())\n                \n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                predictions = (outputs > 0.5).float()\n                total += labels.size(0)\n                correct += (predictions == labels).sum().item()\n        \n        val_loss /= len(val_loader)\n        val_accuracy = correct / total\n        val_losses.append(val_loss)\n        \n        # Update learning rate\n        scheduler.step(val_loss)\n        \n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n        \n        # Print epoch statistics with timing\n        epoch_time = time.time() - epoch_start\n        print(f'Epoch {epoch+1}/{num_epochs} completed in {time.strftime(\"%H:%M:%S\", time.gmtime(epoch_time))}\\n'\n              f'  Train Loss: {train_loss:.4f}, '\n              f'Val Loss: {val_loss:.4f}, '\n              f'Val Accuracy: {val_accuracy:.4f}\\n')\n    \n    # Final timing and output\n    total_time = time.time() - total_start\n    print(f'\\n🔥 Total training completed in {time.strftime(\"%H:%M:%S\", time.gmtime(total_time))}')\n    \n    # Plot training curve\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss')\n    plt.savefig('training_curve.png')\n    \n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.717866Z","iopub.execute_input":"2025-04-14T17:46:54.718078Z","iopub.status.idle":"2025-04-14T17:46:54.737112Z","shell.execute_reply.started":"2025-04-14T17:46:54.718063Z","shell.execute_reply":"2025-04-14T17:46:54.736431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.737716Z","iopub.execute_input":"2025-04-14T17:46:54.737947Z","iopub.status.idle":"2025-04-14T17:46:54.752951Z","shell.execute_reply.started":"2025-04-14T17:46:54.737914Z","shell.execute_reply":"2025-04-14T17:46:54.752304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_emotion(model, text, audio_path, video_path):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)  # Move model to GPU before prediction\n    model.eval()\n    \n    with torch.no_grad():\n        try:\n            # Preprocess inputs\n            audio_spec = AudioEncoder.extract_spectrogram(audio_path).to(device)\n            video_features = VideoEncoder.extract_face_features(video_path).to(device)\n            \n            # Ensure proper batch dimension\n            if audio_spec.dim() == 3:  # [channel, height, width]\n                audio_spec = audio_spec.unsqueeze(0)  # Add batch dimension -> [batch_size, channel, height, width]\n                \n            if video_features.dim() == 3:  # [channel, height, width]\n                video_features = video_features.unsqueeze(0)  # Add batch dimension\n            \n            # Forward pass\n            output = model([text], audio_spec, video_features)\n            \n            # Get prediction\n            if output.dim() == 0:  # If scalar\n                probability = output.item()\n            else:\n                probability = output.squeeze().item()\n                \n            prediction = \"Surprise\" if probability > 0.5 else \"Not Surprise\"\n            \n            return prediction, probability\n            \n        except Exception as e:\n            print(f\"Error during prediction: {e}\")\n            print(f\"Audio spec shape: {audio_spec.shape}\")\n            print(f\"Video features shape: {video_features.shape}\")\n            raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.753911Z","iopub.execute_input":"2025-04-14T17:46:54.754175Z","iopub.status.idle":"2025-04-14T17:46:54.764121Z","shell.execute_reply.started":"2025-04-14T17:46:54.754154Z","shell.execute_reply":"2025-04-14T17:46:54.763522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")  # Should return True[4][5]\nprint(f\"Detected GPU: {torch.cuda.get_device_name(0)}\")  # Should show \"Tesla T4\"[4]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.764725Z","iopub.execute_input":"2025-04-14T17:46:54.764873Z","iopub.status.idle":"2025-04-14T17:46:54.779583Z","shell.execute_reply.started":"2025-04-14T17:46:54.764861Z","shell.execute_reply":"2025-04-14T17:46:54.778977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== MAIN CODE ====================\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Create model\nmodel = MultimodalEmotionRecognition()\nmodel = model.to(device)\n\n# Example usage:\n\n\ndef collate_fn(batch):\n    return {k: v.to(device) for k, v in batch.items()}\n\n\nprint(f\"Model on GPU: {next(model.parameters()).is_cuda}\")\n    \n    \n    # After model creation\nprint(\"Model device check:\")\nprint(f\"Text encoder: {next(model.text_encoder.parameters()).device}\")\nprint(f\"Audio encoder: {next(model.audio_encoder.parameters()).device}\")\nprint(f\"Video encoder: {next(model.video_encoder.parameters()).device}\")\n\n# 1. Training\n# Create datasets and dataloaders\ntrain_dataset = MultimodalEmotionDataset('/kaggle/input/d/rishikant24/bbtsurprise-train-small/bbtSurprise_train_small')\nval_dataset = MultimodalEmotionDataset('/kaggle/input/d/rishikant24/bbtsurprise-val-small/bbtSurprise_val_small')\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=256)\nmodel = train_model(model, train_loader, val_loader, num_epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:46:54.78106Z","iopub.execute_input":"2025-04-14T17:46:54.781547Z","iopub.status.idle":"2025-04-14T18:24:17.7367Z","shell.execute_reply.started":"2025-04-14T17:46:54.781531Z","shell.execute_reply":"2025-04-14T18:24:17.735898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:24:17.737651Z","iopub.execute_input":"2025-04-14T18:24:17.737929Z","iopub.status.idle":"2025-04-14T18:24:17.79664Z","shell.execute_reply.started":"2025-04-14T18:24:17.737896Z","shell.execute_reply":"2025-04-14T18:24:17.79521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:24:17.797295Z","iopub.status.idle":"2025-04-14T18:24:17.797564Z","shell.execute_reply.started":"2025-04-14T18:24:17.797452Z","shell.execute_reply":"2025-04-14T18:24:17.797464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}