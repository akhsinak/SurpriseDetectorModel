{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#### MODEL LOADING CODE################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoFeatureExtractor\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==================== TEXT PROCESSING MODULE ====================\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"distilbert-base-uncased\", hidden_size=768):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model).to(device)\n",
    "\n",
    "        # ❄️ Freeze BERT\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.fc = nn.Linear(hidden_size, 256).to(device)\n",
    "        \n",
    "    def forward(self, texts):\n",
    "        # Tokenize texts\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        # Get text embeddings\n",
    "        outputs = self.model(**inputs)\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
    "        text_features = self.fc(text_features)\n",
    "        return text_features\n",
    "\n",
    "\n",
    "\n",
    "# ==================== AUDIO ENCODER USING HUBERT ====================\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import HUBERT_BASE\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=256):  # input_dim kept for compatibility\n",
    "        super(AudioEncoder, self).__init__()\n",
    "\n",
    "        # Load HuBERT base model from torchaudio\n",
    "        self.hubert_bundle = HUBERT_BASE\n",
    "        self.hubert = self.hubert_bundle.get_model().to(device)\n",
    "\n",
    "        # Freeze HuBERT parameters (can be unfrozen later for fine-tuning)\n",
    "        for param in self.hubert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Projection layer: HuBERT output (768-dim) -> hidden_dim\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(self.hubert_bundle._params['encoder_embed_dim'], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waveforms: Tensor [B, T] or [B, 1, T] (mono)\n",
    "        Returns:\n",
    "            Tensor: [B, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Ensure waveforms are [B, T]\n",
    "        if waveforms.dim() == 3 and waveforms.shape[1] == 1:\n",
    "            waveforms = waveforms.squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features, _ = self.hubert(waveforms)  # [B, T', 768]\n",
    "            pooled = features.mean(dim=1)         # [B, 768]\n",
    "\n",
    "        return self.project(pooled)               # [B, hidden_dim]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_spectrogram(audio_path, target_sr=16000, fixed_len=16000):\n",
    "        \"\"\"\n",
    "        Preprocess .wav audio file into fixed-length waveform tensor.\n",
    "        Args:\n",
    "            audio_path (str): Path to a .wav file\n",
    "            target_sr (int): Target sampling rate\n",
    "            fixed_len (int): Desired number of samples (default: 16000 = 1 second)\n",
    "        Returns:\n",
    "            waveform (Tensor): [1, fixed_len], float32, 16kHz mono\n",
    "        \"\"\"\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "        # Mono conversion\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "        # Resample to 16kHz\n",
    "        if sr != target_sr:\n",
    "            resample = Resample(orig_freq=sr, new_freq=target_sr)\n",
    "            waveform = resample(waveform)\n",
    "    \n",
    "        # Pad or truncate to fixed_len\n",
    "        num_samples = waveform.shape[1]\n",
    "        if num_samples < fixed_len:\n",
    "            pad_size = fixed_len - num_samples\n",
    "            waveform = F.pad(waveform, (0, pad_size))\n",
    "        elif num_samples > fixed_len:\n",
    "            waveform = waveform[:, :fixed_len]\n",
    "    \n",
    "        return waveform.to(device)\n",
    "\n",
    "\n",
    "\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(VideoEncoder, self).__init__()\n",
    "        \n",
    "        # Load pretrained Xception backbone (no classifier)\n",
    "        self.backbone = timm.create_model('xception', pretrained=True, num_classes=0).to(device)\n",
    "        \n",
    "        # Freeze all backbone parameters\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Projection layer\n",
    "        self.fc = nn.Linear(2048, hidden_dim)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 5:\n",
    "            x = x.squeeze(1)  # [B, 1, 3, H, W] → [B, 3, H, W]\n",
    "        features = self.backbone(x)           # [B, 2048]\n",
    "        return self.fc(features)              # [B, hidden_dim]\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_face_features(video_file, num_frames=16):\n",
    "        \"\"\"Extract facial features from video frames\"\"\"\n",
    "        # Initialize face detector\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        \n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        \n",
    "        # Get video properties\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Calculate frame indices to extract\n",
    "        indices = np.linspace(0, frame_count-1, num_frames, dtype=int)\n",
    "        \n",
    "        # Initialize tensor to store face frames\n",
    "        face_frames = torch.zeros((num_frames, 3, 224, 224), device=device)\n",
    "\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            # Set frame position\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            \n",
    "            # Read frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            \n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "            \n",
    "            if len(faces) > 0:\n",
    "                # Extract the largest face\n",
    "                x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])\n",
    "                \n",
    "                # Expand bounding box slightly\n",
    "                x = max(0, x - int(0.1 * w))\n",
    "                y = max(0, y - int(0.1 * h))\n",
    "                w = min(frame.shape[1] - x, int(1.2 * w))\n",
    "                h = min(frame.shape[0] - y, int(1.2 * h))\n",
    "                \n",
    "                # Extract face\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                \n",
    "                # Resize to 224x224\n",
    "                face = cv2.resize(face, (224, 224))\n",
    "                \n",
    "                # Convert to RGB\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                face_tensor = torchvision.transforms.ToTensor()(face)\n",
    "                \n",
    "                # Normalize\n",
    "                face_tensor = torchvision.transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]\n",
    "                )(face_tensor).to(device)\n",
    "                \n",
    "                # Store tensor\n",
    "                face_frames[i] = face_tensor\n",
    "        \n",
    "        # Release video capture\n",
    "        cap.release()\n",
    "        \n",
    "        # Return the mean face features across frames with correct shape\n",
    "        # This will ensure shape is [1, 3, 224, 224]\n",
    "        return face_frames.mean(dim=0).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "# ==================== ATTENTION FUSION MODULE ====================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=256, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        # For PyTorch's MultiheadAttention, inputs should be: [seq_len, batch_size, embed_dim]\n",
    "        # Make sure all inputs are correctly shaped\n",
    "        if query.dim() == 2:\n",
    "            query = query.unsqueeze(0)  # [1, batch_size, embed_dim]\n",
    "        else:\n",
    "            query = query.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n",
    "            \n",
    "        if key.dim() == 2:\n",
    "            key = key.unsqueeze(0)  # [1, batch_size, embed_dim]\n",
    "        elif key.dim() == 3:\n",
    "            key = key.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        if value.dim() == 2:\n",
    "            value = value.unsqueeze(0)  # [1, batch_size, embed_dim]\n",
    "        elif value.dim() == 3:\n",
    "            value = value.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n",
    "\n",
    "\n",
    "        query = query.to(self.device)\n",
    "        key = key.to(self.device)\n",
    "        value = value.to(self.device)\n",
    "        \n",
    "        # Apply multihead attention\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        \n",
    "        # Return to original shape: [batch_size, embed_dim]\n",
    "        return attn_output.transpose(0, 1).squeeze(0)\n",
    "\n",
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.mha = MultiHeadAttention(d_model=hidden_dim).to(self.device)\n",
    "        \n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "        # Create sequence for key and value (3 Ã— [batch_size, feature_dim])\n",
    "        # -> [3, batch_size, feature_dim]\n",
    "        features = torch.stack([text_features.to(self.device), audio_features.to(self.device), video_features.to(self.device)], dim=0)        \n",
    "        # Apply attention to each feature vector as query\n",
    "        # We need to ensure features tensor is correctly shaped for the attention mechanism\n",
    "        text_attn = self.mha(text_features, features, features)\n",
    "        audio_attn = self.mha(audio_features, features, features)\n",
    "        video_attn = self.mha(video_features, features, features)\n",
    "        \n",
    "        # Combine attended features\n",
    "        fused_features = (text_attn + audio_attn + video_attn) / 3\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "# %%\n",
    "class SimplerAttentionFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(SimplerAttentionFusion, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_dim, 3)\n",
    "        \n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "        # Stack features along a new dimension\n",
    "        features = torch.stack([text_features, audio_features, video_features], dim=1)  # [batch_size, 3, hidden_dim]\n",
    "        \n",
    "        # Calculate attention weights (simplified attention)\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        # Use the mean of all features as a query\n",
    "        query = features.mean(dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention_scores = self.attention_weights(query)  # [batch_size, 3]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, 3, 1]\n",
    "        \n",
    "        # Apply attention weights\n",
    "        weighted_features = features * attention_weights  # [batch_size, 3, hidden_dim]\n",
    "        \n",
    "        # Sum over the modalities\n",
    "        fused_features = weighted_features.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        return fused_features\n",
    "    \n",
    "\n",
    "class TransformerFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_layers=2, num_heads=4):\n",
    "        super(TransformerFusion, self).__init__()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_dim*2, \n",
    "            dropout=0.1, \n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.modality_embeddings = nn.Parameter(torch.randn(3, hidden_dim))  # For modality type info\n",
    "\n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "        batch_size = text_features.size(0)\n",
    "        \n",
    "        # Stack features with modality tokens\n",
    "        features = torch.stack([text_features, audio_features, video_features], dim=1)  # [B, 3, D]\n",
    "        \n",
    "        # Add modality embeddings\n",
    "        features = features + self.modality_embeddings.unsqueeze(0)  # [B, 3, D]\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        fused = self.transformer_encoder(features)  # [B, 3, D]\n",
    "        \n",
    "        # Pooling: mean over modalities\n",
    "        fused_output = fused.mean(dim=1)  # [B, D]\n",
    "        \n",
    "        return fused_output\n",
    "    \n",
    "\n",
    "\n",
    "# ==================== FULL MODEL ====================\n",
    "class MultimodalEmotionRecognition(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(MultimodalEmotionRecognition, self).__init__()\n",
    "        \n",
    "        # Encoders for each modality\n",
    "        self.text_encoder = TextEncoder(hidden_size=768, pretrained_model=\"distilbert-base-uncased\")\n",
    "        self.audio_encoder = AudioEncoder(hidden_dim=hidden_dim)\n",
    "        self.video_encoder = VideoEncoder(hidden_dim=hidden_dim)\n",
    "\n",
    "         # Use the simpler attention fusion module\n",
    "        self.fusion = SimplerAttentionFusion(hidden_dim=hidden_dim)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # Binary classification for surprise\n",
    "        \n",
    "    def forward(self, texts, audio_specs, video_frames):\n",
    "        # Fix video input dimensions if needed\n",
    "        if video_frames.dim() == 5:  # [batch_size, 1, 3, height, width]\n",
    "            video_frames = video_frames.squeeze(1)\n",
    "            \n",
    "        # Encode each modality\n",
    "        text_features = self.text_encoder(texts)\n",
    "        audio_features = self.audio_encoder(audio_specs)\n",
    "        video_features = self.video_encoder(video_frames)\n",
    "        \n",
    "        # Fuse features using attention\n",
    "        fused_features = self.fusion(text_features, audio_features, video_features)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.fc(fused_features)\n",
    "        output = torch.sigmoid(output)  # Probability of surprise emotion\n",
    "        \n",
    "        # Ensure output maintains proper dimensions for batch size 1\n",
    "        batch_size = text_features.size(0)\n",
    "        output = output.view(batch_size)  # Reshape to [batch_size]\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def predict_emotion(model, text, audio_path, video_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)  # Move model to GPU before prediction\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Preprocess inputs\n",
    "            audio_spec = AudioEncoder.extract_spectrogram(audio_path).to(device)\n",
    "            video_features = VideoEncoder.extract_face_features(video_path).to(device)\n",
    "            \n",
    "            # Ensure proper batch dimension\n",
    "            if audio_spec.dim() == 3:  # [channel, height, width]\n",
    "                audio_spec = audio_spec.unsqueeze(0)  # Add batch dimension -> [batch_size, channel, height, width]\n",
    "                \n",
    "            if video_features.dim() == 3:  # [channel, height, width]\n",
    "                video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model([text], audio_spec, video_features)\n",
    "            \n",
    "            # Get prediction\n",
    "            if output.dim() == 0:  # If scalar\n",
    "                probability = output.item()\n",
    "            else:\n",
    "                probability = output.squeeze().item()\n",
    "                \n",
    "            prediction = \"Surprise\" if probability > 0.5 else \"Not Surprise\"\n",
    "            \n",
    "            return prediction, probability\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            print(f\"Audio spec shape: {audio_spec.shape}\")\n",
    "            print(f\"Video features shape: {video_features.shape}\")\n",
    "            raise e\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "\n",
    "# 1. Load the saved model\n",
    "def load_trained_model(model_path, device):\n",
    "    # Initialize model architecture\n",
    "    model = MultimodalEmotionRecognition()\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "# 2. Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 3. Load your pretrained model\n",
    "model = load_trained_model('/Users/akhsinak/co/nlp_project/models/best_model_hubert.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efb724",
   "metadata": {},
   "source": [
    "### PIPELINE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30139409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/4] Splitting video/audio/text chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/5] Running inference...\n",
      "JSON saved to: inference_results.json\n",
      "✅ Pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import cv2\n",
    "from datetime import timedelta\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pydub import AudioSegment\n",
    "from google.cloud import speech\n",
    "import pysrt\n",
    "\n",
    "# ================================================\n",
    "# === AUDIO AND TRANSCRIPTION ====================\n",
    "# ================================================\n",
    "\n",
    "def extract_audio_from_mkv(input_file, mp3_output_path):\n",
    "    video_clip = VideoFileClip(input_file)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(mp3_output_path, codec='mp3', bitrate='320k')\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "def convert_mp3_to_wav(mp3_file_path, wav_file_path):\n",
    "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
    "    audio.export(wav_file_path, format=\"wav\")\n",
    "\n",
    "def format_timedelta(td):\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    milliseconds = int(td.microseconds / 1000)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
    "\n",
    "def transcribe_audio_to_srt(wav_file_path, output_srt_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(wav_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "        enable_automatic_punctuation=True\n",
    "    )\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "    srt_content = []\n",
    "    index = 1\n",
    "    for result in response.results:\n",
    "        for alternative in result.alternatives:\n",
    "            start_time_seconds = max(0, result.result_end_time.seconds - len(alternative.transcript.split()) * 0.5)\n",
    "            start_time = timedelta(seconds=start_time_seconds)\n",
    "            end_time = timedelta(seconds=result.result_end_time.seconds)\n",
    "            srt_content.append(f\"{index}\")\n",
    "            srt_content.append(f\"{format_timedelta(start_time)} --> {format_timedelta(end_time)}\")\n",
    "            srt_content.append(alternative.transcript)\n",
    "            srt_content.append(\"\")\n",
    "            index += 1\n",
    "\n",
    "    with open(output_srt_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "        srt_file.write(\"\\n\".join(srt_content))\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# === SPLIT VIDEO/AUDIO/TEXT =====================\n",
    "# ================================================\n",
    "\n",
    "def split_by_srt(video_path, subtitle_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    subs = pysrt.open(subtitle_path)\n",
    "    video = VideoFileClip(video_path)\n",
    "\n",
    "    for sub in subs:\n",
    "        index = sub.index\n",
    "        start_time = sub.start.to_time()\n",
    "        end_time = sub.end.to_time()\n",
    "        start_sec = start_time.hour * 3600 + start_time.minute * 60 + start_time.second + start_time.microsecond / 1e6\n",
    "        end_sec = end_time.hour * 3600 + end_time.minute * 60 + end_time.second + end_time.microsecond / 1e6\n",
    "\n",
    "        clip = video.subclip(start_sec, end_sec)\n",
    "        clip.write_videofile(os.path.join(output_dir, f\"{index}.mp4\"), codec='libx264', audio_codec='aac', verbose=False, logger=None)\n",
    "        clip.audio.write_audiofile(os.path.join(output_dir, f\"{index}.wav\"), verbose=False, logger=None)\n",
    "        with open(os.path.join(output_dir, f\"{index}.txt\"), 'w', encoding='utf-8') as f:\n",
    "            f.write(sub.text)\n",
    "\n",
    "    video.close()\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# === INFERENCE ===================================\n",
    "# ================================================\n",
    "\n",
    "def extract_mid_frame(video_path, output_image_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(f\"Cannot open video: {video_path}\")\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    mid_frame_num = frame_count // 2\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_num)\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        cv2.imwrite(output_image_path, frame)\n",
    "    cap.release()\n",
    "\n",
    "def parse_srt(srt_path):\n",
    "    with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        srt_data = f.read()\n",
    "    entries = []\n",
    "    blocks = re.split(r'\\n\\s*\\n', srt_data.strip())\n",
    "    for block in blocks:\n",
    "        lines = block.strip().splitlines()\n",
    "        if len(lines) >= 3:\n",
    "            id_ = lines[0].strip()\n",
    "            times = lines[1].strip()\n",
    "            caption = \" \".join(line.strip() for line in lines[2:])\n",
    "            start_time, end_time = times.split(\" --> \")\n",
    "            entries.append({\n",
    "                \"id\": id_,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"caption\": caption\n",
    "            })\n",
    "    return entries\n",
    "\n",
    "def batch_infer_srt_format(model, folder_path, srt_path, output_json_path, image_output_dir):\n",
    "    os.makedirs(image_output_dir, exist_ok=True)\n",
    "    srt_entries = parse_srt(srt_path)\n",
    "    final_results = []\n",
    "\n",
    "    for entry in srt_entries:\n",
    "        id_ = entry[\"id\"]\n",
    "        txt_path = os.path.join(folder_path, f\"{id_}.txt\")\n",
    "        wav_path = os.path.join(folder_path, f\"{id_}.wav\")\n",
    "        mp4_path = os.path.join(folder_path, f\"{id_}.mp4\")\n",
    "        image_path = os.path.join(image_output_dir, f\"{id_}_mid.jpg\")\n",
    "\n",
    "        try:\n",
    "            caption = entry[\"caption\"]\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    caption = f.read().strip()\n",
    "\n",
    "            prediction, probability = predict_emotion(model, caption, wav_path, mp4_path)\n",
    "            extract_mid_frame(mp4_path, image_path)\n",
    "\n",
    "            final_results.append({\n",
    "                \"id\": id_,\n",
    "                \"start_time\": entry[\"start_time\"],\n",
    "                \"end_time\": entry[\"end_time\"],\n",
    "                \"caption\": caption,\n",
    "                \"surprise\": \"1\" if prediction.lower() == \"surprise\" else \"0\",\n",
    "                \"probability\": round(probability, 4),\n",
    "                \"midimage\": image_path.replace(\"\\\\\", \"/\")\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ID {id_}: {e}\")\n",
    "\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"JSON saved to: {output_json_path}\")\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# === MAIN DRIVER ================================\n",
    "# ================================================\n",
    "\n",
    "def full_pipeline(video_path, model, output_json_path=\"inference_results.json\"):\n",
    "    base = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    mp3_path = f\"{base}.mp3\"\n",
    "    wav_path = f\"{base}_temp.wav\"\n",
    "    srt_path = f\"{base}.srt\"\n",
    "    output_dir = \"output\"\n",
    "    image_output_dir = \"midimages\"\n",
    "\n",
    "    if not os.path.exists(srt_path):\n",
    "        print(\"[1/4] Extracting audio from video...\")\n",
    "        extract_audio_from_mkv(video_path, mp3_path)\n",
    "\n",
    "        print(\"[2/4] Converting audio to WAV...\")\n",
    "        convert_mp3_to_wav(mp3_path, wav_path)\n",
    "\n",
    "        print(\"[3/4] Generating SRT via transcription...\")\n",
    "        transcribe_audio_to_srt(wav_path, srt_path)\n",
    "        os.remove(wav_path)\n",
    "        \n",
    "    \n",
    "    print(\"[4/4] Splitting video/audio/text chunks...\")\n",
    "    split_by_srt(video_path, srt_path, output_dir)\n",
    "\n",
    "    print(\"[5/5] Running inference...\")\n",
    "    batch_infer_srt_format(model, output_dir, srt_path, output_json_path, image_output_dir)\n",
    "\n",
    "    print(\"✅ Pipeline completed.\")\n",
    "\n",
    "\n",
    "# === CALL THE PIPELINE\n",
    "if __name__ == \"__main__\":\n",
    "    video_input_path = \"sample2.mkv\"  # <-- change this\n",
    "    full_pipeline(video_input_path, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ccca1",
   "metadata": {},
   "source": [
    "### PHASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e5fc2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '4', 'llm_response': {'generated_texts': [\"The answer to which previous utterance most likely caused the surprise 'Is it bad? Did you hear something' is ID 3, because the previous utterances are lighthearted and nothing to really cause surprise, but ID 3 expresses an opinion about ruining lives for a bad movie. In the image, some\"]}}]\n",
      "Updated inference_results_with_cause.json successfully!\n",
      "[('4', '3')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Load the JSON file\n",
    "with open('inference_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Sort data by ID (assuming IDs are numeric or sortable)\n",
    "data.sort(key=lambda x: x['id'])\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "id_map  = {}\n",
    "\n",
    "\n",
    "# Iterate over the data\n",
    "for idx, item in enumerate(data):\n",
    "    if item['surprise'] == \"1\":\n",
    "        # Find up to 4 previous items\n",
    "        history = []\n",
    "        for prev_item in data[max(0, idx-4):idx]:  # look at previous 4\n",
    "            history.append([prev_item['id'], prev_item['caption']])\n",
    "\n",
    "        # Create the output format\n",
    "        entry = {\n",
    "            \"id\": item['id'],\n",
    "            \"prompt\": {\n",
    "                \"target_utterance\": item['caption'],\n",
    "                \"history\": history\n",
    "            }\n",
    "        }\n",
    "        results.append(entry)\n",
    "    id_map[item['id']] = item['midimage']\n",
    "    \n",
    "\n",
    "# Save the results to a new JSON file\n",
    "# with open('surprise_outputs.json', 'w') as f:\n",
    "#     json.dump(results, f, indent=2)\n",
    "\n",
    "# print(\"Done! Saved\", len(results), \"items.\")\n",
    "\n",
    "#llama part\n",
    "def llm_part(prompt, encoded_image):\n",
    "    \n",
    "    url = \"https://cloud.olakrutrim.com/v1/generations/imagetexttotext\"\n",
    "    # Bearer Token\n",
    "    bearer_token = \"yYxd8gYcukg4_LikhFXQFoUpnbfV\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\"\n",
    "    }\n",
    "\n",
    "    # Format history entries\n",
    "    formatted_history = \"\\n\".join([f\"ID {h[0]}: {h[1]}\" for h in prompt['history']])\n",
    "    \n",
    "\n",
    "    user_prompt = \"Explain this image in detail\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"You are given: A target utterance labeled as 'surprise', An image corresponding to the target utterance, A few previous utterances with their IDs. Identify which one previous utterance most likely caused the surprise. Use both the text and the image. Target Utterance: {prompt['target_utterance']} Previous Utterances: {formatted_history}\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    images = [[encoded_image]]\n",
    "    # print(\"message:**  \",messages)\n",
    "    # print(\"1: \",prompt['targetted_utterance'], \" 2. \",formatted_history)\n",
    "\n",
    "\n",
    "    # Define the payload\n",
    "    payload = {\n",
    "        \"modelName\": \"Llama-3.2-11B-Vision-Instruct\",\n",
    "        \"prompts\": messages,\n",
    "        \"images\": images,\n",
    "        \"maxTokens\": 65\n",
    "    }\n",
    "\n",
    "    # Save payload to a file\n",
    "    with open(\"llama-3-2-11b_single_img_payload.json\", 'w') as f:\n",
    "        f.write(json.dumps(payload))\n",
    "\n",
    "    # Send the request\n",
    "    s = time.time()\n",
    "    response = requests.post(url=url, json=payload, headers=headers)\n",
    "    end = time.time() - s\n",
    "\n",
    "    # Output the response\n",
    "    # print(response.text)\n",
    "    # print(\"time: \", end)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "\n",
    "# List to collect all LLM responses\n",
    "llm_responses = []\n",
    "\n",
    "# Iterate through results\n",
    "for item in results:\n",
    "    target_id = item['id']\n",
    "    prompt = item['prompt']\n",
    "    \n",
    "    image_path = id_map.get(target_id)\n",
    "    if image_path:\n",
    "        try:\n",
    "            encoded_image = encode_image(image_path)\n",
    "            response = llm_part(prompt, encoded_image)\n",
    "            # print(\"response: \",response)\n",
    "            if response:\n",
    "                llm_responses.append({\n",
    "                    \"id\": target_id,\n",
    "                    \"llm_response\": response\n",
    "                })\n",
    "            time.sleep(1)  # be nice to the API (depends on rate limits)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ID {target_id}: {e}\")\n",
    "    else:\n",
    "        print(f\"No image found for ID {target_id}\")\n",
    "\n",
    "print(llm_responses)\n",
    "\n",
    "\n",
    "import re\n",
    "# Step 1: Load inference_results.json\n",
    "with open('inference_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Step 2: Create a mapping from id to parsed cause from llm_responses\n",
    "id_to_cause = {}\n",
    "\n",
    "for item in llm_responses:\n",
    "    id_ = item['id']\n",
    "    llm_response_texts = item['llm_response'].get('generated_texts', [])\n",
    "    if llm_response_texts:\n",
    "        text = llm_response_texts[0]\n",
    "        # Extract only the first number using regex\n",
    "        match = re.search(r'\\d+', text)\n",
    "        if match:\n",
    "            cause_id = match.group()\n",
    "            id_to_cause[id_] = cause_id\n",
    "\n",
    "# Step 3: Update data\n",
    "for item in data:\n",
    "    if item.get('surprise') == \"1\":\n",
    "        # Find the cause for this item\n",
    "        cause = id_to_cause.get(item['id'])\n",
    "        if cause:\n",
    "            item['cause'] = cause\n",
    "\n",
    "# Step 4: Save to a new file\n",
    "with open('inference_results_with_cause.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"Updated inference_results_with_cause.json successfully!\")\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Load the updated file\n",
    "with open('inference_results_with_cause.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Step 2: Build the list of (surprise_id, cause_id) pairs\n",
    "pairs = []\n",
    "\n",
    "for item in data:\n",
    "    if item.get('surprise') == \"1\" and 'cause' in item:\n",
    "        surprise_id = item['id']\n",
    "        cause_id = item['cause']\n",
    "        pairs.append((surprise_id, cause_id))\n",
    "\n",
    "# Step 3: Done!\n",
    "print(pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e66487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
